{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "reviews = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "horror movie truly called horror movie scares , suspense , even eerie elements ? think , children corn 666 issac return wants us believe . sixth installment horrible , worn series far worst date . unlike five chapters , children corn 666 confusing , brainless thriller takes psychological horror route rather slasher horror , either way , none movies least bit scary . film follows hannah natalie ramsey teen looking mother gatlin , nebraska , eve 21st birthday . starts daughter desperate search long lost mother turns story hannah first daughter children corn , roam cornfields looking adults murder . understandable film , learn much , issac john franklin led children corn previous chapter , older , strange man , looking hannah fulfill prophecy . supposed make sense . really . start film unclear going , developing characters throwing concrete plot details across table , constantly introducing new characters without personalities slightest hint individuality , sub plots nothing seems main focus film . film runs short 78 minutes , seems vicinity two hours , bleak , slow pacing makes children corn 666 issac return excruciatingly boring . plot holes everywhere tim sulka john franklin unbelievably horrible script , nothing accomplished clear film reaches conclusion . everyone everything involved children corn 666 issac return , namely writers john franklin tim sulka , along director kari skogland , crawl rock , hope one sees horrible work trash . bottom line horrible , horrible , horrible . another attempt revive worn genre falls flat . title ? devil nothing whatsoever film . let pray finale one worst current film series . one worst horror films years .\n",
      "['neg', 'neg', 'neg', 'neg', 'neg', 'neg']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\") + ['\\'', '-', '(', ')', ':', '\"', ';', '--']\n",
    "texts, cats = [], []\n",
    "for text, cat in reviews:\n",
    "    text = ' '.join([w.lower() for w in text if not w.lower() in stop_words])\n",
    "    texts.append(text)\n",
    "    cats.append(cat)\n",
    "print(len(texts), len(cats))\n",
    "print(texts[404])\n",
    "print(cats[4:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 2000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "EMBEDDING_DIM = 32  # 50, 100, 200, 300\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39304 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (2000, 300)\n",
      "[[   0    0    0 ...  192  873  192]\n",
      " [   0    0    0 ...   10  406    3]\n",
      " [   0    0    0 ...  465 1391    1]\n",
      " ...\n",
      " [ 756  218 1007 ... 1609 1362  552]\n",
      " [   0    0    0 ...    1    1   66]\n",
      " [ 331 1397 1505 ...    1  904   71]]\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(type(data), data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2000, 300)\n",
      "Shape of label tensor: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "lb_encoder = LabelEncoder()\n",
    "labels = lb_encoder.fit_transform(cats)\n",
    "labels = to_categorical(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 300) (1700, 2)\n",
      "(300, 300) (300, 2)\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training set and a test set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_test = data[-nb_validation_samples:]\n",
    "y_test = labels[-nb_validation_samples:]\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use keras word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_default = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use GloVe vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('./glove.6B', 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)), 'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_glove = Embedding(len(word_index) + 1,\n",
    "                        EMBEDDING_DIM,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build CNN model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 300, 32)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 300, 32)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 300, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 300, 32)           7200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 300, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 300, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 300, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 300, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 300, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_34 (Conv1D)           (None, 300, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 300, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 300, 2)            66        \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 81,218\n",
      "Trainable params: 80,898\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "use_default = True\n",
    "\n",
    "input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "if use_default:\n",
    "    x = embed_default(input)\n",
    "else:\n",
    "    x = embed_glove(input)\n",
    "\n",
    "net = Dropout(0.2)(x)\n",
    "net = BatchNormalization()(net)\n",
    "\n",
    "net = Conv1D(32, 7, padding='same', activation='relu')(net)\n",
    "net = BatchNormalization()(net)\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
    "net = BatchNormalization()(net)\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
    "net = BatchNormalization()(net)\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\n",
    "net = BatchNormalization()(net)\n",
    "\n",
    "net = Conv1D(2, 1)(net)\n",
    "net = GlobalAveragePooling1D()(net)\n",
    "output = Activation('softmax')(net)\n",
    "model = Model(inputs = input, outputs = output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1530 samples, validate on 170 samples\n",
      "Epoch 1/20\n",
      "1530/1530 [==============================] - 7s 4ms/step - loss: 0.6919 - acc: 0.5216 - val_loss: 0.7159 - val_acc: 0.5294\n",
      "Epoch 2/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.6675 - acc: 0.6085 - val_loss: 0.6807 - val_acc: 0.6059\n",
      "Epoch 3/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.6392 - acc: 0.7105 - val_loss: 0.6625 - val_acc: 0.6353\n",
      "Epoch 4/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.5891 - acc: 0.7549 - val_loss: 0.6357 - val_acc: 0.6353\n",
      "Epoch 5/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.5136 - acc: 0.8033 - val_loss: 0.5365 - val_acc: 0.7235\n",
      "Epoch 6/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.4324 - acc: 0.8484 - val_loss: 0.4856 - val_acc: 0.7588\n",
      "Epoch 7/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.3553 - acc: 0.8771 - val_loss: 0.4797 - val_acc: 0.7471\n",
      "Epoch 8/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.2896 - acc: 0.8961 - val_loss: 0.4720 - val_acc: 0.7588\n",
      "Epoch 9/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.2410 - acc: 0.9222 - val_loss: 0.4709 - val_acc: 0.7588\n",
      "Epoch 10/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.1943 - acc: 0.9490 - val_loss: 0.4516 - val_acc: 0.7824\n",
      "Epoch 11/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.1620 - acc: 0.9621 - val_loss: 0.4555 - val_acc: 0.7824\n",
      "Epoch 12/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.1395 - acc: 0.9739 - val_loss: 0.4458 - val_acc: 0.8176\n",
      "Epoch 13/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.1188 - acc: 0.9850 - val_loss: 0.4274 - val_acc: 0.8059\n",
      "Epoch 14/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.1049 - acc: 0.9902 - val_loss: 0.4273 - val_acc: 0.8059\n",
      "Epoch 15/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.0922 - acc: 0.9941 - val_loss: 0.4144 - val_acc: 0.8118\n",
      "Epoch 16/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.0812 - acc: 0.9967 - val_loss: 0.4155 - val_acc: 0.8176\n",
      "Epoch 17/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.0732 - acc: 0.9980 - val_loss: 0.4213 - val_acc: 0.8235\n",
      "Epoch 18/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.0631 - acc: 0.9987 - val_loss: 0.4293 - val_acc: 0.8235\n",
      "Epoch 19/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.0585 - acc: 0.9987 - val_loss: 0.4412 - val_acc: 0.8294\n",
      "Epoch 20/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.0535 - acc: 0.9993 - val_loss: 0.4554 - val_acc: 0.8412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x43a70208>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=512, epochs=20, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 603us/step\n",
      "loss = 0.40925220559040704\n",
      "acc  = 0.8433333333333334\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('loss = {}\\nacc  = {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build LSTM model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 300, 32)           64000     \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 300, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 120,506\n",
      "Trainable params: 120,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1530 samples, validate on 170 samples\n",
      "Epoch 1/20\n",
      "1530/1530 [==============================] - 6s 4ms/step - loss: 0.6935 - acc: 0.4915 - val_loss: 0.6941 - val_acc: 0.4706\n",
      "Epoch 2/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.6918 - acc: 0.5118 - val_loss: 0.6944 - val_acc: 0.4706\n",
      "Epoch 3/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.6906 - acc: 0.5118 - val_loss: 0.6940 - val_acc: 0.4706\n",
      "Epoch 4/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.6883 - acc: 0.5118 - val_loss: 0.6924 - val_acc: 0.4706\n",
      "Epoch 5/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.6840 - acc: 0.5379 - val_loss: 0.6890 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.6758 - acc: 0.6758 - val_loss: 0.6821 - val_acc: 0.6000\n",
      "Epoch 7/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.6549 - acc: 0.7739 - val_loss: 0.6595 - val_acc: 0.5941\n",
      "Epoch 8/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.6130 - acc: 0.7281 - val_loss: 0.6378 - val_acc: 0.6529\n",
      "Epoch 9/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.6061 - acc: 0.6647 - val_loss: 0.6328 - val_acc: 0.6000\n",
      "Epoch 10/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.5763 - acc: 0.7693 - val_loss: 0.5999 - val_acc: 0.8000\n",
      "Epoch 11/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.5063 - acc: 0.8771 - val_loss: 0.5673 - val_acc: 0.6765\n",
      "Epoch 12/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.4212 - acc: 0.8248 - val_loss: 0.5018 - val_acc: 0.7706\n",
      "Epoch 13/20\n",
      "1530/1530 [==============================] - 5s 3ms/step - loss: 0.3460 - acc: 0.8673 - val_loss: 0.4683 - val_acc: 0.7588\n",
      "Epoch 14/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.2940 - acc: 0.8954 - val_loss: 0.4289 - val_acc: 0.8176\n",
      "Epoch 15/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.2363 - acc: 0.9255 - val_loss: 0.4674 - val_acc: 0.7765\n",
      "Epoch 16/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.2001 - acc: 0.9327 - val_loss: 0.4212 - val_acc: 0.8059\n",
      "Epoch 17/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.1593 - acc: 0.9608 - val_loss: 0.4390 - val_acc: 0.8176\n",
      "Epoch 18/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.1231 - acc: 0.9647 - val_loss: 0.4489 - val_acc: 0.8294\n",
      "Epoch 19/20\n",
      "1530/1530 [==============================] - 4s 3ms/step - loss: 0.0935 - acc: 0.9745 - val_loss: 0.4670 - val_acc: 0.8000\n",
      "Epoch 20/20\n",
      "1530/1530 [==============================] - 4s 2ms/step - loss: 0.0822 - acc: 0.9804 - val_loss: 0.4675 - val_acc: 0.8118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x64322358>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=512, epochs=20, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.42293044368426\n",
      "acc  = 0.8199999992052714\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('loss = {}\\nacc  = {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'kernel': 'linear'} with a score of 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)),category) \n",
    "             for category in movie_reviews.categories() \n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "all_words = all_words.most_common(2000)\n",
    "stop_words = stopwords.words(\"english\")\n",
    "word_features = [w for (w,f) in all_words if w not in stop_words]\n",
    "features = np.zeros([len(documents),len(word_features)],dtype=float)\n",
    "for n in range(len(documents)):\n",
    "    document_words = set(documents[n][0])\n",
    "    for m in range(len(word_features)):\n",
    "        if word_features[m] in document_words:\n",
    "            features[n,m] = 1 # 文件-词集矩阵\n",
    "target = [c for (d,c) in documents]\n",
    "train_set = features[:1500,:]\n",
    "target_train = target[:1500]\n",
    "test_set = features[1500:,:]\n",
    "target_test = target[1500:]\n",
    "\n",
    "# svc= SVC()\n",
    "# svc.fit(train_set,target_train)\n",
    "# pred = svc.predict(test_set)\n",
    "# print(\"支持向量机准确率:\"+str(sum([1 for n in range(len(target_test)) if pred[n]==target_test[n] ])/len(target_test)))\n",
    "tuned_parameters = [{'kernel': ['rbf','poly','linear','sigmoid']}]\n",
    "svm_clf = GridSearchCV(SVC(gamma='auto'), tuned_parameters, cv=10)\n",
    "svm_clf.fit(train_set,target_train)\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (svm_clf.best_params_,svm_clf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
