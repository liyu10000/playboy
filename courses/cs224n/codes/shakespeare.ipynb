{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Like Shakespeare with Machine Learning in Pytorch\n",
    "https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeCwjCNcK23T"
   },
   "source": [
    "### Importing libraries and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PiB1nzhK23V"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M99lF2P2K23e"
   },
   "outputs": [],
   "source": [
    "# Open shakespeare text file and read in data as `text`\n",
    "with open('./shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hs0vmJC_K23j"
   },
   "outputs": [],
   "source": [
    "# encoding the text and map each character to an integer and vice versa\n",
    "\n",
    "# We create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# Encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvwX360DK23o"
   },
   "outputs": [],
   "source": [
    "# Defining method to encode one hot labels\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLhxl0J6K23t"
   },
   "outputs": [],
   "source": [
    "# Defining method to make mini-batches for training\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXEs3Gs3K23x"
   },
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6VbbhSUjK23y",
    "outputId": "a194c904-a753-4a6e-d795-ce2db027659e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1ow4qfCK239"
   },
   "outputs": [],
   "source": [
    "# Declaring the model\n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        #define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        #define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        #get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        #pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPSLNi3fK24A"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad2MoZFuK24C"
   },
   "outputs": [],
   "source": [
    "# Declaring the train method\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JEreTvJ0K24F",
    "outputId": "0818143d-01be-4f60-fd48-52590df13246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(91, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=91, bias=True)\n",
      ")\n",
      "Epoch: 1/20... Step: 50... Loss: 3.2157... Val Loss: 3.2151\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1800... Val Loss: 3.1799\n",
      "Epoch: 1/20... Step: 150... Loss: 2.9496... Val Loss: 2.9182\n",
      "Epoch: 1/20... Step: 200... Loss: 2.6337... Val Loss: 2.6064\n",
      "Epoch: 1/20... Step: 250... Loss: 2.4242... Val Loss: 2.3558\n",
      "Epoch: 1/20... Step: 300... Loss: 2.3145... Val Loss: 2.2236\n",
      "Epoch: 1/20... Step: 350... Loss: 2.1734... Val Loss: 2.1288\n",
      "Epoch: 2/20... Step: 400... Loss: 2.1142... Val Loss: 2.0570\n",
      "Epoch: 2/20... Step: 450... Loss: 2.0474... Val Loss: 1.9911\n",
      "Epoch: 2/20... Step: 500... Loss: 2.0300... Val Loss: 1.9339\n",
      "Epoch: 2/20... Step: 550... Loss: 1.9551... Val Loss: 1.8857\n",
      "Epoch: 2/20... Step: 600... Loss: 1.8984... Val Loss: 1.8431\n",
      "Epoch: 2/20... Step: 650... Loss: 1.8784... Val Loss: 1.8010\n",
      "Epoch: 2/20... Step: 700... Loss: 1.8404... Val Loss: 1.7671\n",
      "Epoch: 2/20... Step: 750... Loss: 1.8103... Val Loss: 1.7381\n",
      "Epoch: 3/20... Step: 800... Loss: 1.7511... Val Loss: 1.7104\n",
      "Epoch: 3/20... Step: 850... Loss: 1.7284... Val Loss: 1.6804\n",
      "Epoch: 3/20... Step: 900... Loss: 1.6931... Val Loss: 1.6582\n",
      "Epoch: 3/20... Step: 950... Loss: 1.7226... Val Loss: 1.6347\n",
      "Epoch: 3/20... Step: 1000... Loss: 1.7048... Val Loss: 1.6160\n",
      "Epoch: 3/20... Step: 1050... Loss: 1.6764... Val Loss: 1.5982\n",
      "Epoch: 3/20... Step: 1100... Loss: 1.6504... Val Loss: 1.5846\n",
      "Epoch: 4/20... Step: 1150... Loss: 1.7459... Val Loss: 1.5793\n",
      "Epoch: 4/20... Step: 1200... Loss: 1.5674... Val Loss: 1.5592\n",
      "Epoch: 4/20... Step: 1250... Loss: 1.5240... Val Loss: 1.5428\n",
      "Epoch: 4/20... Step: 1300... Loss: 1.5678... Val Loss: 1.5308\n",
      "Epoch: 4/20... Step: 1350... Loss: 1.5714... Val Loss: 1.5174\n",
      "Epoch: 4/20... Step: 1400... Loss: 1.5285... Val Loss: 1.5106\n",
      "Epoch: 4/20... Step: 1450... Loss: 1.5339... Val Loss: 1.4997\n",
      "Epoch: 4/20... Step: 1500... Loss: 1.5647... Val Loss: 1.4944\n",
      "Epoch: 5/20... Step: 1550... Loss: 1.5161... Val Loss: 1.4864\n",
      "Epoch: 5/20... Step: 1600... Loss: 1.4782... Val Loss: 1.4786\n",
      "Epoch: 5/20... Step: 1650... Loss: 1.5096... Val Loss: 1.4696\n",
      "Epoch: 5/20... Step: 1700... Loss: 1.5060... Val Loss: 1.4597\n",
      "Epoch: 5/20... Step: 1750... Loss: 1.5075... Val Loss: 1.4533\n",
      "Epoch: 5/20... Step: 1800... Loss: 1.4837... Val Loss: 1.4525\n",
      "Epoch: 5/20... Step: 1850... Loss: 1.4725... Val Loss: 1.4426\n",
      "Epoch: 5/20... Step: 1900... Loss: 1.4659... Val Loss: 1.4365\n",
      "Epoch: 6/20... Step: 1950... Loss: 1.4490... Val Loss: 1.4323\n",
      "Epoch: 6/20... Step: 2000... Loss: 1.4380... Val Loss: 1.4238\n",
      "Epoch: 6/20... Step: 2050... Loss: 1.4178... Val Loss: 1.4157\n",
      "Epoch: 6/20... Step: 2100... Loss: 1.4433... Val Loss: 1.4122\n",
      "Epoch: 6/20... Step: 2150... Loss: 1.4179... Val Loss: 1.4075\n",
      "Epoch: 6/20... Step: 2200... Loss: 1.4224... Val Loss: 1.4051\n",
      "Epoch: 6/20... Step: 2250... Loss: 1.4065... Val Loss: 1.4045\n",
      "Epoch: 7/20... Step: 2300... Loss: 1.4270... Val Loss: 1.3975\n",
      "Epoch: 7/20... Step: 2350... Loss: 1.3897... Val Loss: 1.3960\n",
      "Epoch: 7/20... Step: 2400... Loss: 1.3585... Val Loss: 1.3855\n",
      "Epoch: 7/20... Step: 2450... Loss: 1.3612... Val Loss: 1.3810\n",
      "Epoch: 7/20... Step: 2500... Loss: 1.3809... Val Loss: 1.3777\n",
      "Epoch: 7/20... Step: 2550... Loss: 1.3568... Val Loss: 1.3746\n",
      "Epoch: 7/20... Step: 2600... Loss: 1.3861... Val Loss: 1.3770\n",
      "Epoch: 7/20... Step: 2650... Loss: 1.3698... Val Loss: 1.3721\n",
      "Epoch: 8/20... Step: 2700... Loss: 1.3415... Val Loss: 1.3693\n",
      "Epoch: 8/20... Step: 2750... Loss: 1.3158... Val Loss: 1.3660\n",
      "Epoch: 8/20... Step: 2800... Loss: 1.3361... Val Loss: 1.3607\n",
      "Epoch: 8/20... Step: 2850... Loss: 1.3673... Val Loss: 1.3572\n",
      "Epoch: 8/20... Step: 2900... Loss: 1.3425... Val Loss: 1.3527\n",
      "Epoch: 8/20... Step: 2950... Loss: 1.3317... Val Loss: 1.3517\n",
      "Epoch: 8/20... Step: 3000... Loss: 1.3704... Val Loss: 1.3483\n",
      "Epoch: 8/20... Step: 3050... Loss: 1.3527... Val Loss: 1.3471\n",
      "Epoch: 9/20... Step: 3100... Loss: 1.3654... Val Loss: 1.3443\n",
      "Epoch: 9/20... Step: 3150... Loss: 1.3344... Val Loss: 1.3402\n",
      "Epoch: 9/20... Step: 3200... Loss: 1.2904... Val Loss: 1.3375\n",
      "Epoch: 9/20... Step: 3250... Loss: 1.3468... Val Loss: 1.3362\n",
      "Epoch: 9/20... Step: 3300... Loss: 1.2952... Val Loss: 1.3357\n",
      "Epoch: 9/20... Step: 3350... Loss: 1.3077... Val Loss: 1.3309\n",
      "Epoch: 9/20... Step: 3400... Loss: 1.3248... Val Loss: 1.3406\n",
      "Epoch: 10/20... Step: 3450... Loss: 1.3139... Val Loss: 1.3312\n",
      "Epoch: 10/20... Step: 3500... Loss: 1.3039... Val Loss: 1.3290\n",
      "Epoch: 10/20... Step: 3550... Loss: 1.2640... Val Loss: 1.3256\n",
      "Epoch: 10/20... Step: 3600... Loss: 1.2764... Val Loss: 1.3199\n",
      "Epoch: 10/20... Step: 3650... Loss: 1.3111... Val Loss: 1.3242\n",
      "Epoch: 10/20... Step: 3700... Loss: 1.2870... Val Loss: 1.3206\n",
      "Epoch: 10/20... Step: 3750... Loss: 1.2979... Val Loss: 1.3227\n",
      "Epoch: 10/20... Step: 3800... Loss: 1.2641... Val Loss: 1.3203\n",
      "Epoch: 11/20... Step: 3850... Loss: 1.2809... Val Loss: 1.3209\n",
      "Epoch: 11/20... Step: 3900... Loss: 1.2181... Val Loss: 1.3173\n",
      "Epoch: 11/20... Step: 3950... Loss: 1.3028... Val Loss: 1.3152\n",
      "Epoch: 11/20... Step: 4000... Loss: 1.2757... Val Loss: 1.3089\n",
      "Epoch: 11/20... Step: 4050... Loss: 1.2915... Val Loss: 1.3107\n",
      "Epoch: 11/20... Step: 4100... Loss: 1.2720... Val Loss: 1.3071\n",
      "Epoch: 11/20... Step: 4150... Loss: 1.2895... Val Loss: 1.3063\n",
      "Epoch: 11/20... Step: 4200... Loss: 1.2840... Val Loss: 1.3067\n",
      "Epoch: 12/20... Step: 4250... Loss: 1.2596... Val Loss: 1.3056\n",
      "Epoch: 12/20... Step: 4300... Loss: 1.2621... Val Loss: 1.3011\n",
      "Epoch: 12/20... Step: 4350... Loss: 1.2385... Val Loss: 1.3012\n",
      "Epoch: 12/20... Step: 4400... Loss: 1.3012... Val Loss: 1.2993\n",
      "Epoch: 12/20... Step: 4450... Loss: 1.2657... Val Loss: 1.2986\n",
      "Epoch: 12/20... Step: 4500... Loss: 1.2475... Val Loss: 1.2989\n",
      "Epoch: 12/20... Step: 4550... Loss: 1.2709... Val Loss: 1.3019\n",
      "Epoch: 13/20... Step: 4600... Loss: 1.2673... Val Loss: 1.2952\n",
      "Epoch: 13/20... Step: 4650... Loss: 1.2347... Val Loss: 1.2946\n",
      "Epoch: 13/20... Step: 4700... Loss: 1.2162... Val Loss: 1.2947\n",
      "Epoch: 13/20... Step: 4750... Loss: 1.2538... Val Loss: 1.2913\n",
      "Epoch: 13/20... Step: 4800... Loss: 1.2174... Val Loss: 1.2938\n",
      "Epoch: 13/20... Step: 4850... Loss: 1.2533... Val Loss: 1.2906\n",
      "Epoch: 13/20... Step: 4900... Loss: 1.2456... Val Loss: 1.2902\n",
      "Epoch: 13/20... Step: 4950... Loss: 1.2189... Val Loss: 1.2956\n",
      "Epoch: 14/20... Step: 5000... Loss: 1.2647... Val Loss: 1.2971\n",
      "Epoch: 14/20... Step: 5050... Loss: 1.1730... Val Loss: 1.2911\n",
      "Epoch: 14/20... Step: 5100... Loss: 1.2782... Val Loss: 1.2848\n",
      "Epoch: 14/20... Step: 5150... Loss: 1.2420... Val Loss: 1.2849\n",
      "Epoch: 14/20... Step: 5200... Loss: 1.2663... Val Loss: 1.2871\n",
      "Epoch: 14/20... Step: 5250... Loss: 1.2583... Val Loss: 1.2862\n",
      "Epoch: 14/20... Step: 5300... Loss: 1.2604... Val Loss: 1.2841\n",
      "Epoch: 14/20... Step: 5350... Loss: 1.2315... Val Loss: 1.2886\n",
      "Epoch: 15/20... Step: 5400... Loss: 1.2358... Val Loss: 1.2865\n",
      "Epoch: 15/20... Step: 5450... Loss: 1.2208... Val Loss: 1.2806\n",
      "Epoch: 15/20... Step: 5500... Loss: 1.2026... Val Loss: 1.2806\n",
      "Epoch: 15/20... Step: 5550... Loss: 1.2470... Val Loss: 1.2805\n",
      "Epoch: 15/20... Step: 5600... Loss: 1.2139... Val Loss: 1.2810\n",
      "Epoch: 15/20... Step: 5650... Loss: 1.1943... Val Loss: 1.2757\n",
      "Epoch: 15/20... Step: 5700... Loss: 1.2518... Val Loss: 1.2835\n",
      "Epoch: 16/20... Step: 5750... Loss: 1.2451... Val Loss: 1.2800\n",
      "Epoch: 16/20... Step: 5800... Loss: 1.1793... Val Loss: 1.2816\n",
      "Epoch: 16/20... Step: 5850... Loss: 1.2105... Val Loss: 1.2749\n",
      "Epoch: 16/20... Step: 5900... Loss: 1.1823... Val Loss: 1.2753\n",
      "Epoch: 16/20... Step: 5950... Loss: 1.1903... Val Loss: 1.2764\n",
      "Epoch: 16/20... Step: 6000... Loss: 1.2038... Val Loss: 1.2768\n",
      "Epoch: 16/20... Step: 6050... Loss: 1.2012... Val Loss: 1.2767\n",
      "Epoch: 16/20... Step: 6100... Loss: 1.2043... Val Loss: 1.2772\n",
      "Epoch: 17/20... Step: 6150... Loss: 1.2408... Val Loss: 1.2788\n",
      "Epoch: 17/20... Step: 6200... Loss: 1.1842... Val Loss: 1.2784\n",
      "Epoch: 17/20... Step: 6250... Loss: 1.2314... Val Loss: 1.2736\n",
      "Epoch: 17/20... Step: 6300... Loss: 1.2092... Val Loss: 1.2727\n",
      "Epoch: 17/20... Step: 6350... Loss: 1.2092... Val Loss: 1.2743\n",
      "Epoch: 17/20... Step: 6400... Loss: 1.1580... Val Loss: 1.2691\n",
      "Epoch: 17/20... Step: 6450... Loss: 1.2077... Val Loss: 1.2694\n",
      "Epoch: 17/20... Step: 6500... Loss: 1.1783... Val Loss: 1.2763\n",
      "Epoch: 18/20... Step: 6550... Loss: 1.1966... Val Loss: 1.2718\n",
      "Epoch: 18/20... Step: 6600... Loss: 1.2577... Val Loss: 1.2665\n",
      "Epoch: 18/20... Step: 6650... Loss: 1.1849... Val Loss: 1.2699\n",
      "Epoch: 18/20... Step: 6700... Loss: 1.1984... Val Loss: 1.2692\n",
      "Epoch: 18/20... Step: 6750... Loss: 1.2015... Val Loss: 1.2667\n",
      "Epoch: 18/20... Step: 6800... Loss: 1.1625... Val Loss: 1.2675\n",
      "Epoch: 18/20... Step: 6850... Loss: 1.2076... Val Loss: 1.2695\n",
      "Epoch: 19/20... Step: 6900... Loss: 1.1915... Val Loss: 1.2671\n",
      "Epoch: 19/20... Step: 6950... Loss: 1.1876... Val Loss: 1.2690\n",
      "Epoch: 19/20... Step: 7000... Loss: 1.1815... Val Loss: 1.2656\n",
      "Epoch: 19/20... Step: 7050... Loss: 1.1623... Val Loss: 1.2632\n",
      "Epoch: 19/20... Step: 7100... Loss: 1.1921... Val Loss: 1.2635\n",
      "Epoch: 19/20... Step: 7150... Loss: 1.2034... Val Loss: 1.2637\n",
      "Epoch: 19/20... Step: 7200... Loss: 1.1555... Val Loss: 1.2679\n",
      "Epoch: 19/20... Step: 7250... Loss: 1.1536... Val Loss: 1.2670\n",
      "Epoch: 20/20... Step: 7300... Loss: 1.1989... Val Loss: 1.2682\n",
      "Epoch: 20/20... Step: 7350... Loss: 1.1699... Val Loss: 1.2655\n",
      "Epoch: 20/20... Step: 7400... Loss: 1.2138... Val Loss: 1.2603\n",
      "Epoch: 20/20... Step: 7450... Loss: 1.1699... Val Loss: 1.2661\n",
      "Epoch: 20/20... Step: 7500... Loss: 1.1838... Val Loss: 1.2640\n",
      "Epoch: 20/20... Step: 7550... Loss: 1.1602... Val Loss: 1.2641\n",
      "Epoch: 20/20... Step: 7600... Loss: 1.1888... Val Loss: 1.2633\n",
      "Epoch: 20/20... Step: 7650... Loss: 1.1670... Val Loss: 1.2680\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3qrE3cgK24O"
   },
   "source": [
    "### Generating new Shakespeare text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1k8BBmSK24Q"
   },
   "outputs": [],
   "source": [
    "# Defining a method to generate the next character\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEPbxgoRK24T"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "bJoskQZFK24W",
    "outputId": "f7c034db-7d59-4f30-9542-124ebc0d3910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTHUS,  \n",
      "    Thou didst thou have a prophety and train,\n",
      "    That who was been as sun to be thy friend.\n",
      "    I will to his answer with you, sir,\n",
      "    But that this world was an angry strange\n",
      "    Till thou shalt had my stream, and that's the world.\n",
      "    As thou art as I will to my love so.\n",
      "  CAESAR. She has but man to say the moon will to me.\n",
      "    We shall be mad with the more way the field,\n",
      "    And but her bed were strong what there is so.\n",
      "    They save a bate, between you all these woods.\n",
      "  CASSIUS. It was no fortune and a man of thee.\n",
      "    We have been better stay, by my troth,\n",
      "    So far, I say, and tell me some some fortune\n",
      "    Would be so many and so strengthen and hath made them town\n",
      "    Which we will not divise. They have not been,\n",
      "    All the bastards o' th' ears, which within their\n",
      "    Which shall have blown to bed. I would he shall\n",
      "    I will be both when we will home to thy crush;\n",
      "    Tell me all ours, and being better worth;\n",
      "    Though I am too much still as thou didst see me\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Generating new text\n",
    "print(sample(net, 1000, prime='A', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMDBNWH-MZiP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "shakespeare.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
